{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "import funciones_auxiliares_sql as aux\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style()\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname=javier_rojas user=postgres password=2719\")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_train = \"\"\"\n",
    "CREATE TABLE train_cupid(\n",
    " age INTEGER,\n",
    " height DECIMAL,\n",
    " virgo INTEGER,\n",
    " taurus INTEGER,\n",
    " scorpio INTEGER,\n",
    " pisces INTEGER,\n",
    " libra INTEGER,\n",
    " leo INTEGER,\n",
    " gemini INTEGER,\n",
    " aries INTEGER,\n",
    " aquarius INTEGER,\n",
    " cancer INTEGER,\n",
    " sagittarius INTEGER,\n",
    " asian INTEGER,\n",
    " hispaniclatin INTEGER,\n",
    " black INTEGER,\n",
    " indian INTEGER,\n",
    " pacificislander INTEGER,\n",
    " nativeamerican INTEGER,\n",
    " middleeastern INTEGER,\n",
    " colorado INTEGER,\n",
    " newyork INTEGER,\n",
    " oregon INTEGER,\n",
    " arizona INTEGER,\n",
    " hawaii INTEGER,\n",
    " montana INTEGER,\n",
    " wisconsin INTEGER,\n",
    " virginia INTEGER,\n",
    " spain INTEGER,\n",
    " nevada INTEGER,\n",
    " illinois INTEGER,\n",
    " vietnam INTEGER,\n",
    " ireland INTEGER,\n",
    " louisiana INTEGER,\n",
    " michigan INTEGER,\n",
    " texas INTEGER,\n",
    " unitedkingdom INTEGER,\n",
    " massachusetts INTEGER,\n",
    " northcarolina INTEGER,\n",
    " idaho INTEGER,\n",
    " mississippi INTEGER,\n",
    " newjersey INTEGER,\n",
    " florida INTEGER,\n",
    " minnesota INTEGER,\n",
    " georgia INTEGER,\n",
    " utah INTEGER,\n",
    " washington INTEGER,\n",
    " westvirginia INTEGER,\n",
    " connecticut INTEGER,\n",
    " tennessee INTEGER,\n",
    " rhodeisland INTEGER,\n",
    " districtofcolumbia INTEGER,\n",
    " canada INTEGER,\n",
    " missouri INTEGER,\n",
    " germany INTEGER,\n",
    " pennsylvania INTEGER,\n",
    " netherlands INTEGER,\n",
    " switzerland INTEGER,\n",
    " mexico INTEGER,\n",
    " ohio INTEGER,\n",
    " agnosticism INTEGER,\n",
    " atheism INTEGER,\n",
    " catholicism INTEGER,\n",
    " buddhism INTEGER,\n",
    " judaism INTEGER,\n",
    " hinduism INTEGER,\n",
    " islam INTEGER,\n",
    " pro_dogs DECIMAL,\n",
    " pro_cats DECIMAL,\n",
    " spanish INTEGER,\n",
    " chinese INTEGER,\n",
    " french INTEGER,\n",
    " german INTEGER,\n",
    " single INTEGER,\n",
    " seeing_someone INTEGER,\n",
    " available INTEGER,\n",
    " employed INTEGER,\n",
    " income_between_25_50 INTEGER,\n",
    " income_between_50_75 INTEGER,\n",
    " income_over_75 INTEGER,\n",
    " drugs_often INTEGER,\n",
    " drugs_sometimes INTEGER,\n",
    " drinks_notatall INTEGER,\n",
    " drinks_often INTEGER,\n",
    " drinks_rarely INTEGER,\n",
    " drinks_socially INTEGER,\n",
    " drinks_veryoften INTEGER,\n",
    " orientation_gay INTEGER,\n",
    " orientation_straight INTEGER,\n",
    " sex_m INTEGER,\n",
    " smokes_sometimes INTEGER,\n",
    " smokes_tryingtoquit INTEGER,\n",
    " smokes_whendrinking INTEGER,\n",
    " smokes_yes INTEGER,\n",
    " body_type_overweight INTEGER,\n",
    " body_type_regular INTEGER,\n",
    " education_high_school INTEGER,\n",
    " education_undergrad_university INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "query_test = \"\"\"\n",
    "CREATE TABLE test_cupid(\n",
    " age INTEGER,\n",
    " height DECIMAL,\n",
    " virgo INTEGER,\n",
    " taurus INTEGER,\n",
    " scorpio INTEGER,\n",
    " pisces INTEGER,\n",
    " libra INTEGER,\n",
    " leo INTEGER,\n",
    " gemini INTEGER,\n",
    " aries INTEGER,\n",
    " aquarius INTEGER,\n",
    " cancer INTEGER,\n",
    " sagittarius INTEGER,\n",
    " asian INTEGER,\n",
    " hispaniclatin INTEGER,\n",
    " black INTEGER,\n",
    " indian INTEGER,\n",
    " pacificislander INTEGER,\n",
    " nativeamerican INTEGER,\n",
    " middleeastern INTEGER,\n",
    " colorado INTEGER,\n",
    " newyork INTEGER,\n",
    " oregon INTEGER,\n",
    " arizona INTEGER,\n",
    " hawaii INTEGER,\n",
    " montana INTEGER,\n",
    " wisconsin INTEGER,\n",
    " virginia INTEGER,\n",
    " spain INTEGER,\n",
    " nevada INTEGER,\n",
    " illinois INTEGER,\n",
    " vietnam INTEGER,\n",
    " ireland INTEGER,\n",
    " louisiana INTEGER,\n",
    " michigan INTEGER,\n",
    " texas INTEGER,\n",
    " unitedkingdom INTEGER,\n",
    " massachusetts INTEGER,\n",
    " northcarolina INTEGER,\n",
    " idaho INTEGER,\n",
    " mississippi INTEGER,\n",
    " newjersey INTEGER,\n",
    " florida INTEGER,\n",
    " minnesota INTEGER,\n",
    " georgia INTEGER,\n",
    " utah INTEGER,\n",
    " washington INTEGER,\n",
    " westvirginia INTEGER,\n",
    " connecticut INTEGER,\n",
    " tennessee INTEGER,\n",
    " rhodeisland INTEGER,\n",
    " districtofcolumbia INTEGER,\n",
    " canada INTEGER,\n",
    " missouri INTEGER,\n",
    " germany INTEGER,\n",
    " pennsylvania INTEGER,\n",
    " netherlands INTEGER,\n",
    " switzerland INTEGER,\n",
    " mexico INTEGER,\n",
    " ohio INTEGER,\n",
    " agnosticism INTEGER,\n",
    " atheism INTEGER,\n",
    " catholicism INTEGER,\n",
    " buddhism INTEGER,\n",
    " judaism INTEGER,\n",
    " hinduism INTEGER,\n",
    " islam INTEGER,\n",
    " pro_dogs DECIMAL,\n",
    " pro_cats DECIMAL,\n",
    " spanish INTEGER,\n",
    " chinese INTEGER,\n",
    " french INTEGER,\n",
    " german INTEGER,\n",
    " single INTEGER,\n",
    " seeing_someone INTEGER,\n",
    " available INTEGER,\n",
    " employed INTEGER,\n",
    " income_between_25_50 INTEGER,\n",
    " income_between_50_75 INTEGER,\n",
    " income_over_75 INTEGER,\n",
    " drugs_often INTEGER,\n",
    " drugs_sometimes INTEGER,\n",
    " drinks_notatall INTEGER,\n",
    " drinks_often INTEGER,\n",
    " drinks_rarely INTEGER,\n",
    " drinks_socially INTEGER,\n",
    " drinks_veryoften INTEGER,\n",
    " orientation_gay INTEGER,\n",
    " orientation_straight INTEGER,\n",
    " sex_m INTEGER,\n",
    " smokes_sometimes INTEGER,\n",
    " smokes_tryingtoquit INTEGER,\n",
    " smokes_whendrinking INTEGER,\n",
    " smokes_yes INTEGER,\n",
    " body_type_overweight INTEGER,\n",
    " body_type_regular INTEGER,\n",
    " education_high_school INTEGER,\n",
    " education_undergrad_university INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query_train)\n",
    "cursor.execute(query_test)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.insertar_csv_a_tabla_postgres(\"input/train_cupid.csv\", \"train_cupid\", cursor=cursor)\n",
    "aux.insertar_csv_a_tabla_postgres(\"input/test_cupid.csv\", \"test_cupid\", cursor=cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNAS = [\n",
    "    \"age\",\n",
    "    \"height\",\n",
    "    \"virgo\",\n",
    "    \"taurus\",\n",
    "    \"scorpio\",\n",
    "    \"pisces\",\n",
    "    \"libra\",\n",
    "    \"leo\",\n",
    "    \"gemini\",\n",
    "    \"aries\",\n",
    "    \"aquarius\",\n",
    "    \"cancer\",\n",
    "    \"sagittarius\",\n",
    "    \"asian\",\n",
    "    \"hispaniclatin\",\n",
    "    \"black\",\n",
    "    \"indian\",\n",
    "    \"pacificislander\",\n",
    "    \"nativeamerican\",\n",
    "    \"middleeastern\",\n",
    "    \"colorado\",\n",
    "    \"newyork\",\n",
    "    \"oregon\",\n",
    "    \"arizona\",\n",
    "    \"hawaii\",\n",
    "    \"montana\",\n",
    "    \"wisconsin\",\n",
    "    \"virginia\",\n",
    "    \"spain\",\n",
    "    \"nevada\",\n",
    "    \"illinois\",\n",
    "    \"vietnam\",\n",
    "    \"ireland\",\n",
    "    \"louisiana\",\n",
    "    \"michigan\",\n",
    "    \"texas\",\n",
    "    \"unitedkingdom\",\n",
    "    \"massachusetts\",\n",
    "    \"northcarolina\",\n",
    "    \"idaho\",\n",
    "    \"mississippi\",\n",
    "    \"newjersey\",\n",
    "    \"florida\",\n",
    "    \"minnesota\",\n",
    "    \"georgia\",\n",
    "    \"utah\",\n",
    "    \"washington\",\n",
    "    \"westvirginia\",\n",
    "    \"connecticut\",\n",
    "    \"tennessee\",\n",
    "    \"rhodeisland\",\n",
    "    \"districtofcolumbia\",\n",
    "    \"canada\",\n",
    "    \"missouri\",\n",
    "    \"germany\",\n",
    "    \"pennsylvania\",\n",
    "    \"netherlands\",\n",
    "    \"switzerland\",\n",
    "    \"mexico\",\n",
    "    \"ohio\",\n",
    "    \"agnosticism\",\n",
    "    \"atheism\",\n",
    "    \"catholicism\",\n",
    "    \"buddhism\",\n",
    "    \"judaism\",\n",
    "    \"hinduism\",\n",
    "    \"islam\",\n",
    "    \"pro_dogs\",\n",
    "    \"pro_cats\",\n",
    "    \"spanish\",\n",
    "    \"chinese\",\n",
    "    \"french\",\n",
    "    \"german\",\n",
    "    \"single\",\n",
    "    \"seeing_someone\",\n",
    "    \"available\",\n",
    "    \"employed\",\n",
    "    \"income_between_25_50\",\n",
    "    \"income_between_50_75\",\n",
    "    \"income_over_75\",\n",
    "    \"drugs_often\",\n",
    "    \"drugs_sometimes\",\n",
    "    \"drinks_notatall\",\n",
    "    \"drinks_often\",\n",
    "    \"drinks_rarely\",\n",
    "    \"drinks_socially\",\n",
    "    \"drinks_veryoften\",\n",
    "    \"orientation_gay\",\n",
    "    \"orientation_straight\",\n",
    "    \"sex_m\",\n",
    "    \"smokes_sometimes\",\n",
    "    \"smokes_tryingtoquit\",\n",
    "    \"smokes_whendrinking\",\n",
    "    \"smokes_yes\",\n",
    "    \"body_type_overweight\",\n",
    "    \"body_type_regular\",\n",
    "    \"education_high_school\",\n",
    "    \"education_undergrad_university\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT * FROM train_cupid;\")\n",
    "train = pd.DataFrame(cursor.fetchall(), columns=COLUMNAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectores_objetivos = [\"single\", \"seeing_someone\", \"available\"]\n",
    "modelos = {\n",
    "    \"gradient_boost\": GradientBoostingClassifier,\n",
    "    \"ada_boost\": AdaBoostClassifier,\n",
    "    \"random_forest\": RandomForestClassifier,\n",
    "    \"svc\": SVC,\n",
    "    \"decision_tree\": DecisionTreeClassifier,\n",
    "    \"logistic\": LogisticRegression,\n",
    "    \"naive_bayes\": BernoulliNB,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_modelos = {}\n",
    "for var_dependiente in vectores_objetivos:\n",
    "    for nombre_modelo, tipo_modelo in modelos.items():\n",
    "        X = train.drop(columns=var_dependiente)\n",
    "        y = train[var_dependiente]\n",
    "\n",
    "        modelo_entrenado = tipo_modelo().fit(X, y)\n",
    "\n",
    "        id_modelo = f\"{nombre_modelo}_pred_{var_dependiente}\"\n",
    "\n",
    "        resultados_modelos[id_modelo] = modelo_entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"modelos.pickle\", \"wb\") as f:\n",
    "    pickle.dump(resultados_modelos, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT * FROM test_cupid;\")\n",
    "test = pd.DataFrame(cursor.fetchall(), columns=COLUMNAS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = test.query(\n",
    "    \"(atheism == 1) and (asian == 1) and (employed == 1) and (pro_dogs == 1.0) and (chinese == 1)\"\n",
    ")\n",
    "\n",
    "query_2 = test.query(\n",
    "    \"(income_over_75 == 1) and (french == 1) and (german == 1) and (orientation_straight == 1) and (newyork == 1)\"\n",
    ")\n",
    "\n",
    "query_3 = test.query(\n",
    "    \"(education_undergrad_university == 1) and (body_type_regular == 1) and (pro_dogs == 1.0) and (employed == 1)\"\n",
    ")\n",
    "\n",
    "query_4 = test.query(\n",
    "    \"(taurus == 1) and (indian == 1) and (washington == 1) and (income_between_50_75 == 1) and (hinduism == 1)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conjunto_queries = [query_1, query_2, query_3, query_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testeando a gradient_boost_pred_single con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86         4\n",
      "           1       0.96      1.00      0.98        26\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.98      0.88      0.92        30\n",
      "weighted avg       0.97      0.97      0.96        30\n",
      " \n",
      "\n",
      "Testeando a ada_boost_pred_single con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86         4\n",
      "           1       0.96      1.00      0.98        26\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.98      0.88      0.92        30\n",
      "weighted avg       0.97      0.97      0.96        30\n",
      " \n",
      "\n",
      "Testeando a random_forest_pred_single con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86         4\n",
      "           1       0.96      1.00      0.98        26\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.98      0.88      0.92        30\n",
      "weighted avg       0.97      0.97      0.96        30\n",
      " \n",
      "\n",
      "Testeando a svc_pred_single con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.87      1.00      0.93        26\n",
      "\n",
      "    accuracy                           0.87        30\n",
      "   macro avg       0.43      0.50      0.46        30\n",
      "weighted avg       0.75      0.87      0.80        30\n",
      " \n",
      "\n",
      "Testeando a decision_tree_pred_single con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60         4\n",
      "           1       0.96      0.88      0.92        26\n",
      "\n",
      "    accuracy                           0.87        30\n",
      "   macro avg       0.73      0.82      0.76        30\n",
      "weighted avg       0.90      0.87      0.88        30\n",
      " \n",
      "\n",
      "Testeando a logistic_pred_single con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86         4\n",
      "           1       0.96      1.00      0.98        26\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.98      0.88      0.92        30\n",
      "weighted avg       0.97      0.97      0.96        30\n",
      " \n",
      "\n",
      "Testeando a naive_bayes_pred_single con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86         4\n",
      "           1       0.96      1.00      0.98        26\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.98      0.88      0.92        30\n",
      "weighted avg       0.97      0.97      0.96        30\n",
      " \n",
      "\n",
      "Testeando a gradient_boost_pred_seeing_someone con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      " \n",
      "\n",
      "Testeando a ada_boost_pred_seeing_someone con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      " \n",
      "\n",
      "Testeando a random_forest_pred_seeing_someone con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      " \n",
      "\n",
      "Testeando a svc_pred_seeing_someone con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        27\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.45      0.50      0.47        30\n",
      "weighted avg       0.81      0.90      0.85        30\n",
      " \n",
      "\n",
      "Testeando a decision_tree_pred_seeing_someone con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      " \n",
      "\n",
      "Testeando a logistic_pred_seeing_someone con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      " \n",
      "\n",
      "Testeando a naive_bayes_pred_seeing_someone con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      " \n",
      "\n",
      "Testeando a gradient_boost_pred_available con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      " \n",
      "\n",
      "Testeando a ada_boost_pred_available con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      " \n",
      "\n",
      "Testeando a random_forest_pred_available con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      " \n",
      "\n",
      "Testeando a svc_pred_available con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        27\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.45      0.50      0.47        30\n",
      "weighted avg       0.81      0.90      0.85        30\n",
      " \n",
      "\n",
      "Testeando a decision_tree_pred_available con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      " \n",
      "\n",
      "Testeando a logistic_pred_available con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      " \n",
      "\n",
      "Testeando a naive_bayes_pred_available con la query 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        27\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      " \n",
      "\n",
      "Testeando a gradient_boost_pred_single con la query 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 97)) while a minimum of 1 is required by GradientBoostingClassifier.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m nombre_modelo, modelo \u001b[39min\u001b[39;00m modelos_a_entrenar\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTesteando a \u001b[39m\u001b[39m{\u001b[39;00mnombre_modelo\u001b[39m}\u001b[39;00m\u001b[39m con la query \u001b[39m\u001b[39m{\u001b[39;00mi \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     yhat \u001b[39m=\u001b[39m modelo\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[0;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(classification_report(y_test, yhat), \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ppizarro\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1308\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m   1294\u001b[0m     \u001b[39m\"\"\"Predict class for X.\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m \n\u001b[0;32m   1296\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1306\u001b[0m \u001b[39m        The predicted values.\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1308\u001b[0m     raw_predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(X)\n\u001b[0;32m   1309\u001b[0m     encoded_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loss\u001b[39m.\u001b[39m_raw_prediction_to_decision(raw_predictions)\n\u001b[0;32m   1310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(encoded_labels, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ppizarro\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1261\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecision_function\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m   1243\u001b[0m     \u001b[39m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m \n\u001b[0;32m   1245\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1259\u001b[0m \u001b[39m        array of shape (n_samples,).\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1261\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1262\u001b[0m         X, dtype\u001b[39m=\u001b[39;49mDTYPE, order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1263\u001b[0m     )\n\u001b[0;32m   1264\u001b[0m     raw_predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raw_predict(X)\n\u001b[0;32m   1265\u001b[0m     \u001b[39mif\u001b[39;00m raw_predictions\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ppizarro\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    547\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\ppizarro\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:931\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[0;32m    930\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 931\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    932\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    934\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    935\u001b[0m         )\n\u001b[0;32m    937\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    938\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 97)) while a minimum of 1 is required by GradientBoostingClassifier."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "for i, query in enumerate(conjunto_queries):\n",
    "    for vector_objetivo in vectores_objetivos:\n",
    "        X_test = query.drop(columns=vector_objetivo)\n",
    "        y_test = query[vector_objetivo]\n",
    "\n",
    "        modelos_a_entrenar = {\n",
    "            nombre_modelo: modelo\n",
    "            for nombre_modelo, modelo in resultados_modelos.items()\n",
    "            if vector_objetivo in nombre_modelo\n",
    "        }\n",
    "\n",
    "        for nombre_modelo, modelo in modelos_a_entrenar.items():\n",
    "            print(f\"Testeando a {nombre_modelo} con la query {i + 1}\")\n",
    "            yhat = modelo.predict(X_test)\n",
    "\n",
    "            print(classification_report(y_test, yhat), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gradient_boost_pred_single': GradientBoostingClassifier(),\n",
       " 'ada_boost_pred_single': AdaBoostClassifier(),\n",
       " 'random_forest_pred_single': RandomForestClassifier(),\n",
       " 'svc_pred_single': SVC(),\n",
       " 'decision_tree_pred_single': DecisionTreeClassifier(),\n",
       " 'logistic_pred_single': LogisticRegression(),\n",
       " 'naive_bayes_pred_single': BernoulliNB(),\n",
       " 'gradient_boost_pred_seeing_someone': GradientBoostingClassifier(),\n",
       " 'ada_boost_pred_seeing_someone': AdaBoostClassifier(),\n",
       " 'random_forest_pred_seeing_someone': RandomForestClassifier(),\n",
       " 'svc_pred_seeing_someone': SVC(),\n",
       " 'decision_tree_pred_seeing_someone': DecisionTreeClassifier(),\n",
       " 'logistic_pred_seeing_someone': LogisticRegression(),\n",
       " 'naive_bayes_pred_seeing_someone': BernoulliNB(),\n",
       " 'gradient_boost_pred_available': GradientBoostingClassifier(),\n",
       " 'ada_boost_pred_available': AdaBoostClassifier(),\n",
       " 'random_forest_pred_available': RandomForestClassifier(),\n",
       " 'svc_pred_available': SVC(),\n",
       " 'decision_tree_pred_available': DecisionTreeClassifier(),\n",
       " 'logistic_pred_available': LogisticRegression(),\n",
       " 'naive_bayes_pred_available': BernoulliNB()}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados_modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_actual = os.getcwd()\n",
    "archivos_dump = list(map(lambda x: f'{dir_actual}/{x}', glob.glob('input/dump/*.csv')))\n",
    "\n",
    "dfs = (pd.read_csv(f) for f in archivos_dump)\n",
    "df = pd.concat(dfs).drop(columns='Unnamed: 0')\n",
    "df.columns = ['artist', 'genre', 'song_name', 'lyrics']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las 5000 palabras mas frecuentes en la base de datos son: ['000' '10' '100' ... 'zoo' 'zoom' 'zulu']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "vectorizer_fit = vectorizer.fit_transform(df['lyrics'])\n",
    "\n",
    "words = vectorizer.get_feature_names_out()\n",
    "freqs = vectorizer_fit.toarray().sum(axis=0)\n",
    "\n",
    "print(f'Las 5000 palabras mas frecuentes en la base de datos son: {words}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = [5, 10, 15]\n",
    "learning_decay = [0.7, 0.5]\n",
    "\n",
    "grid = {\n",
    "    'n_components': n_components,\n",
    "    'learning_decay': learning_decay\n",
    "}\n",
    "\n",
    "modelo_grilla = GridSearchCV(LatentDirichletAllocation(learning_method='online'), \n",
    "                             param_grid=grid, verbose=5, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = CountVectorizer(stop_words='english', max_features=5000, max_df=0.1).fit_transform(df['lyrics'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV 1/5] END learning_decay=0.7, n_components=5;, score=-1163788.469 total time=  35.0s\n",
      "[CV 2/5] END learning_decay=0.7, n_components=5;, score=-1473371.251 total time=  33.3s\n",
      "[CV 3/5] END learning_decay=0.7, n_components=5;, score=-1321940.552 total time=  34.2s\n",
      "[CV 4/5] END learning_decay=0.7, n_components=5;, score=-1171146.482 total time=  32.0s\n",
      "[CV 5/5] END learning_decay=0.7, n_components=5;, score=-1475996.457 total time=  30.5s\n",
      "[CV 1/5] END learning_decay=0.7, n_components=10;, score=-1180431.711 total time=  35.4s\n",
      "[CV 2/5] END learning_decay=0.7, n_components=10;, score=-1488246.542 total time=  35.0s\n",
      "[CV 3/5] END learning_decay=0.7, n_components=10;, score=-1341744.658 total time=  34.5s\n",
      "[CV 4/5] END learning_decay=0.7, n_components=10;, score=-1192856.468 total time=  34.9s\n",
      "[CV 5/5] END learning_decay=0.7, n_components=10;, score=-1493181.077 total time=  32.2s\n",
      "[CV 1/5] END learning_decay=0.7, n_components=15;, score=-1191991.383 total time=  35.9s\n",
      "[CV 2/5] END learning_decay=0.7, n_components=15;, score=-1499690.035 total time=  35.8s\n",
      "[CV 3/5] END learning_decay=0.7, n_components=15;, score=-1351902.993 total time=  32.7s\n",
      "[CV 4/5] END learning_decay=0.7, n_components=15;, score=-1210053.624 total time=  32.3s\n",
      "[CV 5/5] END learning_decay=0.7, n_components=15;, score=-1506700.706 total time=  30.1s\n",
      "[CV 1/5] END learning_decay=0.5, n_components=5;, score=-1167279.792 total time=  23.5s\n",
      "[CV 2/5] END learning_decay=0.5, n_components=5;, score=-1473713.224 total time=  20.9s\n",
      "[CV 3/5] END learning_decay=0.5, n_components=5;, score=-1320590.402 total time=  21.5s\n",
      "[CV 4/5] END learning_decay=0.5, n_components=5;, score=-1177988.119 total time=  24.0s\n",
      "[CV 5/5] END learning_decay=0.5, n_components=5;, score=-1476135.129 total time=  22.9s\n",
      "[CV 1/5] END learning_decay=0.5, n_components=10;, score=-1181386.725 total time=  23.8s\n",
      "[CV 2/5] END learning_decay=0.5, n_components=10;, score=-1491505.083 total time=  22.7s\n",
      "[CV 3/5] END learning_decay=0.5, n_components=10;, score=-1340504.120 total time=  23.2s\n",
      "[CV 4/5] END learning_decay=0.5, n_components=10;, score=-1190555.016 total time=  22.9s\n",
      "[CV 5/5] END learning_decay=0.5, n_components=10;, score=-1496732.090 total time=  22.2s\n",
      "[CV 1/5] END learning_decay=0.5, n_components=15;, score=-1190882.965 total time=  23.7s\n",
      "[CV 2/5] END learning_decay=0.5, n_components=15;, score=-1500766.732 total time=  23.7s\n",
      "[CV 3/5] END learning_decay=0.5, n_components=15;, score=-1351100.794 total time=  24.5s\n",
      "[CV 4/5] END learning_decay=0.5, n_components=15;, score=-1200480.667 total time=  25.0s\n",
      "[CV 5/5] END learning_decay=0.5, n_components=15;, score=-1502952.495 total time=  23.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=LatentDirichletAllocation(learning_method='online'),\n",
       "             param_grid={'learning_decay': [0.7, 0.5],\n",
       "                         'n_components': [5, 10, 15]},\n",
       "             verbose=5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_grilla.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_decay': 0.7, 'n_components': 5}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_grilla.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', n_components=5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mejor_learning_decay = 0.7\n",
    "mejor_n_components = 5\n",
    "best_lda = LatentDirichletAllocation(n_components=mejor_n_components, \n",
    "                                     learning_decay=mejor_learning_decay,\n",
    "                                     learning_method='online')\n",
    "best_lda.fit(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tópico: 1\n",
      "niggas umm niggaz lyrics rap funk goons wiz bird garbage humanity cheat beans vibrations chopped\n",
      "tópico: 2\n",
      "ones ah ladies tony rock example yesterday dang whores nah hail roll round daisy ba\n",
      "tópico: 3\n",
      "mall fuse funky creation damn froze helps shadow helped empire boats hatred tool buyin alot\n",
      "tópico: 4\n",
      "goodness dies workout west thousand benz remind tan gun maybe young tiny knot looked heartbeat\n",
      "tópico: 5\n",
      "pages sunrise familiar dressed bein losing forsake slang fresh drill fee moments alien darling figures\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic_name in enumerate(best_lda.components_):\n",
    "# para cada tópico\n",
    "    print(\"tópico: {}\".format(topic_id + 1))\n",
    "    print(\" \".join([vectorizer.get_feature_names_out()[i] for i in\n",
    "    topic_name.argsort()[:-15 - 1: -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a677eee07fae7a05dc0065b321405b89ac79028a918e58c71b412840ee16c11e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

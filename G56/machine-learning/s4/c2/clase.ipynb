{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging y Random Forest\n",
    "\n",
    "- Son un tipo de modelo de ensamble paralelo. Esto significa que se generan muchos arboles al mismo tiempo, y que estan al mismo nivel de jerarquia. Luego, se obtiene la prediccion de cada uno, y a traves de democracia se predice una regresion o clasificacion!\n",
    "\n",
    "- (Un modelo de ensamble secuencial es cuando se genera un primer arbol, y se obtiene el error de este. Luego, el segundo arbol aprende del error del anterior y lo intenta mejorar, y asi.)\n",
    "\n",
    "- El unico problema que tiene es que genera una representacion unica de los datos\n",
    "\n",
    "# Limitantes de modelos de instancia unica\n",
    "\n",
    "Recordemos que los arboles tienen alta tendencia a sobreajustar mucho (onda, al nivel de tener un accuracy del 0.99 o 1)\n",
    "\n",
    "# Bagging\n",
    "\n",
    "Es la metodologia para evitar sobreajustar los modelos de arboles. Es un mecanismo:\n",
    "\n",
    "1. Se tienen los datos de entrenamiento\n",
    "2. Se obtiene una submuestra de los datos, pero es una muestra con reemplazo. Esto significa que obtengo una submuestra, y luego la devuelvo al dataset. Por lo tanto, en los muestreos que se hagan es posible que se repitan datos!. Este proceso se llama bootstrap.\n",
    "3. Este proceso de bootstrapping se repite por la cantidad de modelos a generar.\n",
    "\n",
    "Un ejemplo del bootstrapping es el siguiente:\n",
    "\n",
    "1. Tener unos datos\n",
    "2. Sacar N veces una submuestra de los datos\n",
    "3. Calculo para cada submuestra su media y la anoto\n",
    "4. Termino de sacar las n submuestras\n",
    "5. Grafico las medias. Ya que es un muestreo aleatorio, se obtendra una distribucion normal!. De esta distribucion normal obtengo la media y esa es la media de mis datos!. \n",
    "\n",
    "Esta tecnica es mucho mejor para obtener la media, ya que se tiene toda la versatilidad de los datos, y se evita el sesgo!.\n",
    "\n",
    "En el caso del machine learning o Random Forest, el bootstrapping es el siguiente:\n",
    "\n",
    "1. Se tienen los datos de entrenamiento\n",
    "2. Se obtienen N submuestras (Ej: 10000, 100, 10)\n",
    "3. Para cada submuestra se entrena/genera un arbol de decision\n",
    "4. Una vez hecho todos los arboles, se obtiene la prediccion para cada uno de los arboles.\n",
    "5. Finalmente, la prediccion final sera la mayoria de la clase/regresion predicha. Tambien se puede calcular la probabilidad de obtener clases/regresiones. En este caso, la probabilidad es el promedio de la probabilidad de cada arbol!\n",
    "\n",
    "## Bagging con Columnas\n",
    "\n",
    "Ademas del mecanismo mencionado previamente para el bootstrapping, tambien se puede hacer un bootstrapping de las columnas (variables). Por lo tanto, se puede hacer un subsample de las filas, como tambien de las columnas!\n",
    "\n",
    "Aqui menciono que para hacer un arbol random con esta metodologia de bootstrapping hay que ordenar las columnas...?\n",
    "\n",
    "## Numero de Columnas a seleccionar\n",
    "\n",
    "Pasa que cuando se ocupan todas las columnas para hacer los arboles existen problemas de correlacion entre los arboles generados. Para evitar esto, se seleccionan menos atributos por arbol. La cantidad de atributos a seleccionar son:\n",
    "\n",
    "- log(n_atributos)\n",
    "- sqrt(n_atributos)\n",
    "\n",
    "Esto, porque se hicieron investigaciones para ver cuales eran las mejores!\n",
    "\n",
    "# Out of Bag\n",
    "\n",
    "- Recordemos que con el Bagging se obtiene una submuestra. Los datos que quedan fuera de la submuestra es utilizada para hacer la validacion el arbol hecho. O sea, es como un pseudo Cross Validation, ya que se genera un subdato de entrenamiento y otro de validacion para calcular las metricas!.\n",
    "\n",
    "- Esto se implementa, ya que hacer un fit de un Random Forest con un grilla/cross validation es extremadamente costoso e incluso prohibitivo (el PC evita que se haga)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "import lec7_graphs as afx\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cadata.csv', header=1).drop(columns='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_MedianIncome'] = np.log(df['MedianIncome'])\n",
    "df['log_MedianHouseValue'] = np.log(df['MedianHouseValue'])\n",
    "df = df.drop(columns=['MedianHouseValue', 'MedianIncome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns='log_MedianHouseValue')\n",
    "y = df['log_MedianHouseValue']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparemtros de RandomForest\n",
    "\n",
    "- bootstraping: Con True se hace subsample con reemplazo\n",
    "- oob_score: Es el Out of Bag Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bagging_model = BaggingRegressor(random_state=11238).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.17423180412322567\n",
      "MSE: 0.06333404302179182\n",
      "R2: 0.8039087422091201\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "print(f'MAE: {mean_absolute_error(y_test, bagging_model.predict(X_test))}')\n",
    "print(f'MSE: {mean_squared_error(y_test, bagging_model.predict(X_test))}')\n",
    "print(f'R2: {r2_score(y_test, bagging_model.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingRegressor(random_state=11238)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

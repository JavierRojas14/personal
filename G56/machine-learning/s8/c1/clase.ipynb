{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Recurrentes (Recurrent Neural Network)\n",
    "\n",
    "Como que se desenrollan, pero no entendi nada jejejej\n",
    "\n",
    "La cosa es que son buenos para predecir series de tiempo! Al final, toma en cuenta la informacion que se tiene actualmente, y se almacena en la \"memoria\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from keras.models import Sequential # Se instancia el tipo de modelo\n",
    "from keras.layers import SimpleRNN # Se instancia el tipo de capa\n",
    "from keras.optimizers import Adam # Se instancia el optimizador\n",
    "\n",
    "\n",
    "lag = 3 # Son las veces que vamos a ir hacia atras\n",
    "\n",
    "model_simplernn = Sequential()\n",
    "\n",
    "model_simplernn.add(\n",
    "    SimpleRNN(units=1,\n",
    "              input_shape=(1, lag),\n",
    "              return_sequences=True)\n",
    ")\n",
    "\n",
    "model_simplernn.compile(loss='mean_squared_error',\n",
    "                        optimizer=Adam())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Una de las precauciones que hay tener es evitar que se desvanezca el gradiente (caiga en valores demasiado pequenos y que el computador sea incapaz de manejarlo. Ej: 0.2 ^ 1000). Tambien puede explotar (en los libros le llaman explotar), cuando los gradientes son demasiado grandes y tampoco se pueden manejar (Ej: 2 ^ 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory\n",
    "\n",
    "- Las redes neuronales recurrentes guarda toda la informacion que tiene previamente.\n",
    "\n",
    "- Guardar toda la informacion genera inestabilidad!\n",
    "\n",
    "- Los gradientes pueden explotar, porque se pueden tener gradientes muy pequenos. Y si se guardan todos los valores, entonces se llega a un valor infinitamente pequeno, o uno infinitamente grande!\n",
    "\n",
    "- En el LSTM se plantea que es innecesario tener absolutamente toda la informacion previa!\n",
    "\n",
    "Absolutamente no entiendo lel\n",
    "\n",
    "- Forget: Indica la informacion que se retiene y la que se olvida!\n",
    "- Input\n",
    "- Gate\n",
    "- Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units\n",
    "\n",
    "El problema de LSTM aumenta la complejidad de los modelos, debido a las compuertas que tienen\n",
    "\n",
    "Para resolver eso, tenemos las Gated Recurrent Units! Que sintetiza las compuertas en 2 solamente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio de Temperaturas\n",
    "\n",
    "Aqui grafica las temperaturas a lo largo de los anios y se puede ver que hay un patron de temperaturas!\n",
    "\n",
    "Al final estos modelos creo que son muy buenos para predecir datos que vengan de series de tiempo.\n",
    "En este caso, los datos tienen correlacion entre si!. Un modelo de redes neuronales convencional\n",
    "seria incapaz de capturar la relacion que tienen los datos entre si. Debido a esto, se utilizan modelos\n",
    "donde exista alguna memoria, o algo por el estilo!\n",
    "\n",
    "\n",
    "- Por lo tanto, es mucho mejor utilizar una red neuronal que sea capaz de procesar series de tiempos\n",
    "o relacionados entre si.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM\n",
    "from keras.callbacks import History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()\n",
    "lag = 3\n",
    "batch_1 = 1\n",
    "inputs = 4\n",
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(\n",
    "    LSTM(\n",
    "    inputs, input_shape=(1, lag),\n",
    "    activation='tanh',\n",
    "    recurrent_activation='sigmoid',\n",
    "    return_sequences=True,\n",
    "    name='Capa_LSTM'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Recurrentes (Recurrent Neural Network)\n",
    "\n",
    "Como que se desenrollan, pero no entendi nada jejejej\n",
    "\n",
    "La cosa es que son buenos para predecir series de tiempo! Al final, toma en cuenta la informacion que se tiene actualmente, y se almacena en la \"memoria\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from keras.models import Sequential # Se instancia el tipo de modelo\n",
    "from keras.layers import SimpleRNN # Se instancia el tipo de capa\n",
    "from keras.optimizers import Adam # Se instancia el optimizador\n",
    "\n",
    "\n",
    "lag = 3 # Son las veces que vamos a ir hacia atras\n",
    "\n",
    "model_simplernn = Sequential()\n",
    "\n",
    "model_simplernn.add(\n",
    "    SimpleRNN(units=1,\n",
    "              input_shape=(1, lag),\n",
    "              return_sequences=True)\n",
    ")\n",
    "\n",
    "model_simplernn.compile(loss='mean_squared_error',\n",
    "                        optimizer=Adam())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Una de las precauciones que hay tener es evitar que se desvanezca el gradiente (caiga en valores demasiado pequenos y que el computador sea incapaz de manejarlo. Ej: 0.2 ^ 1000). Tambien puede explotar (en los libros le llaman explotar), cuando los gradientes son demasiado grandes y tampoco se pueden manejar (Ej: 2 ^ 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory\n",
    "\n",
    "- Las redes neuronales recurrentes guarda toda la informacion que tiene previamente.\n",
    "\n",
    "- Guardar toda la informacion genera inestabilidad!\n",
    "\n",
    "- Los gradientes pueden explotar, porque se pueden tener gradientes muy pequenos. Y si se guardan todos los valores, entonces se llega a un valor infinitamente pequeno, o uno infinitamente grande!\n",
    "\n",
    "- En el LSTM se plantea que es innecesario tener absolutamente toda la informacion previa!\n",
    "\n",
    "Absolutamente no entiendo lel\n",
    "\n",
    "- Forget: Indica la informacion que se retiene y la que se olvida!\n",
    "- Input\n",
    "- Gate\n",
    "- Output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units\n",
    "\n",
    "El problema de LSTM aumenta la complejidad de los modelos, debido a las compuertas que tienen\n",
    "\n",
    "Para resolver eso, tenemos las Gated Recurrent Units! Que sintetiza las compuertas en 2 solamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

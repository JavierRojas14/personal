{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elementos conformantes\n",
    "\n",
    "Todos los elementos que hacen una red neuronal\n",
    "\n",
    "## 1. Neurona\n",
    "\n",
    "## 2. Capa\n",
    "\n",
    "Conjunto de neuronas\n",
    "\n",
    "## 3. Capa oculta\n",
    "\n",
    "Son las capas de neuronas que estan internamente dentro de una red neuronal.\n",
    "\n",
    "## 4. Sesgo y Pesos!\n",
    "\n",
    "Hay sesgos y pesos en cada neurona, que multiplican al input!. La funcion del sesgo y el peso responde a la forma de:\n",
    "\n",
    "y = mx + n\n",
    "\n",
    "Donde m es el sesgo, y n es el peso.\n",
    "\n",
    "Por lo tanto, antes de pasar los datos a la funcion de activacion, los datos son transformados con el peso y el sesgo!. Esto finalmente cambia la forma de la funcion de activacion!\n",
    "\n",
    "- El sesgo (m) modifica la curvatura de la funcion que tiene cada neurona!\n",
    "- El peso (n) modifica desde donde parte la funcion que tiene cada neurona!\n",
    "\n",
    "Al final es como cuando se modifica una funcion con la pendiente y el n. Pendiente modifica la inclinacion de la funcion, mientras que n en donde parte.\n",
    "\n",
    "## 5. Funciones de Activacion\n",
    "\n",
    "- Lineal: Una de las variantes de esta funcion es la funcion identidad!\n",
    "- Sigmoide\n",
    "- Tangente\n",
    "- RELU (Funcion Lineal Unitaria Rectificada): Si es negativo, entonces 0, si es sobre 0, entonces es\n",
    "el mismo valor!. Permite capturar solamente \n",
    "- Softmax: Permite resolver el problema de clasificacion de varias clases. Hace que todas las clases\n",
    "sumen 1!\n",
    "\n",
    "## 6. Capa de Salida\n",
    "\n",
    "Es la capa de salida. Es una neurona que recolecta diversos inputs, y genera el ultimo output!\n",
    "\n",
    "# Entrenamiento y optimizacion\n",
    "\n",
    "- Optimizador: De Gradiente Estocastico\n",
    "- Epocas o iteraciones: Indica cuantas veces queremos iterar para optimizar nuestros parametros en\n",
    "la red neuronal\n",
    "\n",
    "# Flujo de una red neronal\n",
    "\n",
    "1. Recibe los datos\n",
    "2. Se calculan los valores para cada neurona en cada capa\n",
    "3. Y se propaga!\n",
    "\n",
    "En el diagrama se entiende super bien la verdad jejejej\n",
    "\n",
    "O sea:\n",
    "\n",
    "1. Obtener la suma ponderada a nivel de cada capa (O sea, cada dato por el peso de la capa (o la pendinete))\n",
    "2. Agregar el sesgo a la suma ponderada\n",
    "3. El resultado evaluarlo en la funcion de activacion!\n",
    "4. El resultado de la capa/neurona es el resultado de la suma ponderada en la funcion de activacion! :D Super nice jejejej\n",
    "\n",
    "# Primera gran decision\n",
    "\n",
    "- Notemos que la capa de entrada y de salida es siempre la misma. La capa de entrada es la cantidad\n",
    "de atributos de nuestros datos de entrenamiento. La capa de salida corresponde a nuestro vector\n",
    "objetivo, o la cantidad de clases a predecir!\n",
    "\n",
    "- Sin embargo, para las capas ocultas? Cuantas neuronas ocupar? Cuantas capas?\n",
    "\n",
    "- Para la cantidad de neuronas por capa:\n",
    "    - Ocupar el promedio de atributos entre la capa de entrada y la de salida\n",
    "    - Ocupar 2/3 de neuronas en una capa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordar harto mini batch! Porque es el mejor tipo de stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    Dense(units=10, input_dim=5, activation=\"relu\", name=\"hidden1\")\n",
    ")  # Si nuestros datos tienen\n",
    "# 5 variables, entonces se pone un input dim de 5!\n",
    "model.add(Dense(units=10, activation=\"relu\", name=\"hidden2\")) # Con units se declaran que se utilizaran 10 neuronas por cada\n",
    "model.add(Dense(units=1, activation=\"sigmoid\", name=\"output\"))\n",
    "model.compile(optimizer=SGD(learning_rate=1), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(X, y, epochs=50, batch_size=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style()\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "X, y = make_moons(n_samples=500, random_state=11238, noise=0.15)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=11238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_red_neuronal(X_train, y_train, n_neurons=10):\n",
    "    tmp_model = Sequential()\n",
    "    tmp_model.add(\n",
    "    Dense(n_neurons,\n",
    "          input_dim= X_train.shape[1],\n",
    "          kernel_initializer='glorot_normal'),\n",
    "          activation='relu',\n",
    "          name='hidden1'\n",
    ")\n",
    "\n",
    "    tmp_model.add(\n",
    "    Dense(\n",
    "    1,\n",
    "    kernel_initializer='glorot_normal',\n",
    "    activation='sigmoid',\n",
    "    name='output'\n",
    "    )\n",
    ")\n",
    "\n",
    "    tmp_model.compile(\n",
    "    optimizer=SGD(learning_rate=1),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "    tmp_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=100,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "    diccionario = tmp_model.history.history\n",
    "\n",
    "    return tmp_model, diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales\n",
    "\n",
    "Aqui estuvimos viendo el problema de otpimizacion, y la utilizacion de gradientes/pendientes para minimizar las funciones de perdidas.\n",
    "\n",
    "Recordemos que las funciones de perdida (como el Error cuadratico medio) siempre lo queremos minimizar en problemas de regresion. Por lo tanto, queremos buscar los parametros optimos (En una regresion lineal, serian la pendiente y el intercepto) que minimicen a la funcion de perdida.\n",
    "\n",
    "Por ejemplo, para encontrar el X minimo de una funcion cuadrada\n",
    "\n",
    "1. $x2$\n",
    "2. Obtenemos la derivada (gradiente en este caso), que seria $2x$\n",
    "3. Ahora, queremos ver para donde va la gradiente, y como minimizarlo.\n",
    "4. Para moverse, se utiliza la forma: $x2 + 2x * learning_rate$\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "Si estamos en el \n",
    "\n",
    "- X = 2\n",
    "- y = 4 \n",
    "- con una pendiente de 4. \n",
    "\n",
    "Ahora, queremos minimizar la funcion, por lo que tenemos que movernos en la direccion opuesta del\n",
    "gradiente. Por lo tanto, podemos tomar un learning rate de -0.05. Y podemos evaluar el nuevo punto\n",
    "en el que estamos\n",
    "\n",
    "$Nuevo punto = x2 + 2x * learning_rate*\n",
    "\n",
    "$Nuevo punto = 4 + 4 * -0.05$\n",
    "\n",
    "$Nuevo punto = 3.8$\n",
    "\n",
    "Por lo tanto, minimizamos la funcion! Entonces, tenemos que iterar hasta obtener un minimo y sha\n",
    "esta (pero esto es horriblemente costoso)\n",
    "\n",
    "# Problemas del Gradient Descent\n",
    "\n",
    "- Es costoso computacionalmente\n",
    "- Puede variar por el learning rate\n",
    "- Ocupa todo el dataset\n",
    "- Se puede quedar estancado en un minimo local\n",
    "\n",
    "# Arreglos del Gradient Descent - Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Una forma de mejorar el desempeno es con el Stochastic Gradient Descent. Es mas \"nervioso\" (ya\n",
    "que surca de forma mas aleatoria el espacio de la funcion)\n",
    "\n",
    "- No ocupa todo el dataset para entrenar y buscar el minimo\n",
    "- Evalua cada uno de los puntos de los datos de entrenamiento, y de forma aleatoria\n",
    "- Esto permite surcar mucho mas el espacio de datos, por lo que se puede ver de forma mas rapida\n",
    "donde puede estar el potencial minimo!\n",
    "\n",
    "## Variantes del SGD - Mini Batching\n",
    "\n",
    "- Hace otras cosas pero no cache\n",
    "\n",
    "## Variantes actuales - Momentum\n",
    "\n",
    "- Es muy similar al Stochastic Descent\n",
    "- Pero en este caso se actualiza el learning rate segun el desempeno de la busqueda del minimo\n",
    "de la funcion de error. O sea, si el modelo esta cayendo dentro de los mismos datos y dentro\n",
    "del mismo surco, entonces modifica el learning rate para que sea menos nervioso.\n",
    "\n",
    "## Variantes actuales - Adagrad\n",
    "\n",
    "- Tambien actualiza el learning rate a lo largo de la optimizacion del gradiente\n",
    "- Incluso, puede solventar la eleccion de un learning rate muy malo. \n",
    "- Sin embargo, puede que se llegue a un learning rate que se estanque\n",
    "\n",
    "## Variantes actuales - Adadelta o RMSprop\n",
    "\n",
    "- Similar a Adagrad\n",
    "- Optimiza el learning rate en cuanto al desempeno anterior y a la CANTIDAD de learnings \n",
    "rate anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
